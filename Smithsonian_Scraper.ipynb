{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Proxy Checker\n",
        "\n",
        "This section checks whether a proxy is being used and if it is working. It compares the IP address obtained with and without the proxy."
      ],
      "metadata": {
        "id": "ilI5RSwKJbhY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmoV38Gv0b7E",
        "outputId": "1781fc07-167f-4963-bf95-0cb19ae022a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently not using proxy\n"
          ]
        }
      ],
      "source": [
        "# Proxy Checker\n",
        "\n",
        "import requests\n",
        "\n",
        "url = \"http://ident.me/\"\n",
        "\n",
        "proxies = \"\"  # Fill in your proxy details\n",
        "\n",
        "#Check Proxy IP\n",
        "try:\n",
        "    proxy_ip = requests.get(url, proxies={\"http\": proxies, \"https\": proxies})\n",
        "    proxy_ip.raise_for_status()\n",
        "except requests.RequestException as e:\n",
        "    print(f\"Proxy check failed: {e}\")\n",
        "    proxy_ip = None\n",
        "\n",
        "#Check for Real IP\n",
        "try:\n",
        "    real_ip = requests.get(url)\n",
        "    real_ip.raise_for_status()\n",
        "except requests.RequestException as e:\n",
        "    print(f\"Failed to get real IP: {e}\")\n",
        "    real_ip = None\n",
        "\n",
        "if proxy_ip and real_ip:\n",
        "    if proxy_ip.text == real_ip.text:\n",
        "        print(\"Currently not using proxy\")\n",
        "    else:\n",
        "        print(\"Proxy working, current IP:\", proxy_ip.text)\n",
        "else:\n",
        "    print(\"Unable to determine proxy status\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Smithsonian Web Scraper\n",
        "\n",
        "This cell scrapes data from the American History section of the Smithsonian website. It extracts the titles, images, and topics of items and saves the data into a CSV file.\n"
      ],
      "metadata": {
        "id": "bzJ3R3joLvMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "data = []\n",
        "\n",
        "base_url = \"https://americanhistory.si.edu/collections/search?page=\"\n",
        "pages = 1 # can be changed till max 1001 as only 1001 unique pages exist in this website\n",
        "\n",
        "for i in range(pages):\n",
        "    current_url = base_url + str(i)\n",
        "\n",
        "    proxies = \"\"  # Update your proxy settings if needed\n",
        "\n",
        "    try:\n",
        "        page = requests.get(current_url, proxies=proxies)\n",
        "        page.raise_for_status()  # Ensure that the request was successful\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Failed to retrieve page {i}: {e}\")\n",
        "        continue\n",
        "\n",
        "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
        "\n",
        "    links = soup.find_all(\"h3\", class_=\"c-card__title\")\n",
        "\n",
        "    for link in links:\n",
        "        item = {}\n",
        "        title_tag = link.find(\"a\")\n",
        "\n",
        "        if title_tag:\n",
        "            item['Title'] = title_tag.get_text()\n",
        "            url_B = \"https://americanhistory.si.edu\" + title_tag.attrs.get(\"href\", \"\")\n",
        "\n",
        "            try:\n",
        "                pageB = requests.get(url_B, proxies=proxies)\n",
        "                pageB.raise_for_status()  # Ensure that the request was successful\n",
        "            except requests.RequestException as e:\n",
        "                print(f\"Failed to retrieve details page {url_B}: {e}\")\n",
        "                continue\n",
        "\n",
        "            soupB = BeautifulSoup(pageB.text, \"html.parser\")\n",
        "\n",
        "            # Initialize the photo variable\n",
        "            photo = None\n",
        "\n",
        "            # Find the grandparent element with the specific required class\n",
        "            photo_grandparent = soupB.find(\"div\", class_=\"media-container media--no-openaccess has-ids type--slideshowhtml\")\n",
        "\n",
        "            if photo_grandparent:\n",
        "                a_tag = photo_grandparent.find(\"a\")\n",
        "                if a_tag:\n",
        "                    img_tag = a_tag.find(\"img\")\n",
        "                    if img_tag and \"src\" in img_tag.attrs:\n",
        "                        photo = img_tag.attrs[\"src\"]\n",
        "\n",
        "            if photo:\n",
        "                item['Picture'] = photo\n",
        "            else:\n",
        "                item['Picture'] = \"Photo not found\"\n",
        "\n",
        "            # Find the p element with the specific required class\n",
        "            topic_container = soupB.find(\"p\", class_=\"edan-record__meta-detail freetext:topic\")\n",
        "            if topic_container:\n",
        "                # Extract all of the \"span\" elements within the p element with required class\n",
        "                topic_spans = topic_container.find_all(\"span\", class_=\"edan-record__meta-content\")\n",
        "                topics = [span.get_text(strip=True) for span in topic_spans]\n",
        "                item['Topic'] = topics\n",
        "            else:\n",
        "                item['Topic'] = \"Topics were not found\"\n",
        "\n",
        "            data.append(item)\n",
        "        else:\n",
        "            print(\"No title found for a link\")\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "#Saving data to CSV\n",
        "df.to_csv(\"data.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "b1KfasfHLvim"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}
